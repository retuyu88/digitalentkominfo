{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of [30 1] Layer-wise Pretraining - ADF",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/retuyu88/digitalentkominfo/blob/master/Copy_of_%5B30_1%5D_Layer_wise_Pretraining_ADF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfwbLGN0zdro",
        "colab_type": "text"
      },
      "source": [
        "<img src = \"https://i.imgur.com/UjutVJd.jpg\" align = \"center\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7DHWa10jRCa",
        "colab_type": "text"
      },
      "source": [
        "# Perkembangan Deep Learning\n",
        "\n",
        "Meskipun ditemukan banyak kelemahan, perkembangan untuk memperbaiki Jaringan Saraf Tiruan terus dilakukan hingga mulai populernya istilah ***Deep Learning*** pada tahun 2006 saat Geoff Hinton memperkenalkan teknik baru untuk melatih Jaringan Saraf Tiruan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x6hH5uqeyOI",
        "colab_type": "text"
      },
      "source": [
        "# Layer-wise Unsupervised Pretraining\n",
        "\n",
        "Pada tahun 2006, Geoffrey Hinton and Ruth Salakhutdinov berhasil melatih Jaringan dengan arsitektur layer lebih dari 10 layer dengan cara melatih layer-per-layer satu per satu menggunakan ***Restricted Boltzmann Machine*** hingga bobot layer mampu merepresentasikan input data, baru kemudian menggabungkan layer-layer yang telah dilatih tersebut menjadi satu kesatuan Jaringan. Setelah tertumpuk, Jaringan dapat dilatih dengan Back-Propagation tanpa khawatir mengalami *Vanishing Gradient Problem*\n",
        "\n",
        "![RBM](https://image.ibb.co/eHq6Ae/mlenewimage14.png)\n",
        "\n",
        "Implementasi RBM cukup rumit, bahkan menggunakan Tensorflow. Untuk itu mari kita coba proses yang sama dengan metode yang setara namun lebih mudah, yaitu Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZHWERrPeyOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, Activation\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8yXxkSceyOM",
        "colab_type": "text"
      },
      "source": [
        "# Autoencoder\n",
        "\n",
        "Autoencoder adalah Jaringan Saraf yang dilatih dengan tujuan untuk mengkopi input menjadi output. Jaringan Autoencoder terdiri dari dua bagian: Encoder dan Decoder. Awalnya, Autoencoder digunakan untuk melakukan reduksi dimensi atau *feature learning* yang untuk beberapa kasus berhasil mereduksi dimensi lebih baik daripada PCA.\n",
        "![autoencoder](https://blog.keras.io/img/ae/autoencoder_schema.jpg)\n",
        "Kali ini kita juga akan gunakan dataset Fashion MNIST langsung dari dataset tensorflow. Maka load kembali dataset\n",
        "\n",
        "Gambar: blog.keras.io "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu4T7EU_js9B",
        "colab_type": "text"
      },
      "source": [
        "## Load Fashion MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_OSSoxOeyON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWTYLxJXeyOR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the text labels\n",
        "fashion_mnist_labels = [\"T-shirt/top\",  # index 0\n",
        "                        \"Trouser\",      # index 1\n",
        "                        \"Pullover\",     # index 2 \n",
        "                        \"Dress\",        # index 3 \n",
        "                        \"Coat\",         # index 4\n",
        "                        \"Sandal\",       # index 5\n",
        "                        \"Shirt\",        # index 6 \n",
        "                        \"Sneaker\",      # index 7 \n",
        "                        \"Bag\",          # index 8 \n",
        "                        \"Ankle boot\"]   # index 9\n",
        "num_class = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiSph75teyOV",
        "colab_type": "text"
      },
      "source": [
        "Setelah kita unduh dataset, mari kita coba tampilkan tampilan beberapa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0XsIFOqeyOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(2,10,figsize=(15,4.5))\n",
        "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
        "for j in range(0,2):\n",
        "    for i in range(0, 10):\n",
        "        ax[j,i].imshow(x_train[i+j*10], cmap='gray')\n",
        "        ax[j,i].set_title(y_train[i+j*10])\n",
        "        ax[j,i].axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ36XxP9eyOb",
        "colab_type": "text"
      },
      "source": [
        "Kemudian kita lakukan preprocessing terhadap data untuk meratakan data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pml165hGeyOc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "n_input = x_train.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGHyMJp_eyOg",
        "colab_type": "text"
      },
      "source": [
        "# First Layer Pretrain AutoEncoder\n",
        "Untuk membuat suatu jaringan Autoencoder, maka kita gunakan sebuah layer Dense dengan input ukuran data dan output ukuran reduksi dimesi.\n",
        "\n",
        "Setelah itu kita tumpuk dengan sebuah layer Dense lagi yang menerima input berukuran hasil reduksi dimensi dan output kembali ke ukuran data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCDfi40HeyOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoding_dim = 500 \n",
        "\n",
        "input_img = Input(shape=(n_input,))\n",
        "\n",
        "encoded = Dense(encoding_dim, activation='sigmoid')(input_img)  # layer 1\n",
        "\n",
        "decoded = Dense(n_input, activation='sigmoid')(encoded)         # layer 2\n",
        "\n",
        "# satukan model\n",
        "autoencoder = Model(input_img, decoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwxsoxeNeyOl",
        "colab_type": "text"
      },
      "source": [
        "### Train AutoEncoder\n",
        "Mari kita coba melatih jaringan dengan 30 epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ue3eCImeyOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "autoencoder.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=30,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNZq2RxieyOq",
        "colab_type": "text"
      },
      "source": [
        "### Test Encoder\n",
        "Untuk melakukan prediksi, maka mari kita buat model Encoder dan Decoder yang masing-masing merupakan setengah dari model yang sudah dilatih"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69I3Dq_eeyOr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Model(input_img, encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA0nzDB-eyOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded_input = Input(shape=(encoding_dim,))\n",
        "decoder_layer = autoencoder.layers[-1]\n",
        "decoder = Model(encoded_input, decoder_layer(encoded_input))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqAIvH7ReyOx",
        "colab_type": "text"
      },
      "source": [
        "Mari kita coba untuk meng-encode data test dan kemudian mencoba merekonstruksi kembali hasil encode tersebut"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sByfDdpmeyOz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded_imgs = encoder.predict(x_test)\n",
        "decoded_imgs = decoder.predict(encoded_imgs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOJ-lRVkeyO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n = 10  # how many digits we will display\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28,28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28,28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTj_zYhEeyO4",
        "colab_type": "text"
      },
      "source": [
        "# Second Layer Pretrain AutoEncoder\n",
        "Konsep dari *layer-wise pretrain model* adalah dengan membuat jaringan baru lagi yang menerima input hasil reduksi dimensi, dan dilatih untuk mereduksi dimensi lebih jauh.\n",
        "\n",
        "Untuk itu kita bangung model autoencoder kedua sebagai berikut"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zjww6ExheyO6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoding_dim2 = 200\n",
        "\n",
        "input_img2 = Input(shape=(encoding_dim,))\n",
        "encoded2 = Dense(encoding_dim2, activation='sigmoid')(input_img2)\n",
        "decoded2 = Dense(encoding_dim, activation='sigmoid')(encoded2)\n",
        "\n",
        "autoencoder2 = Model(input_img2, decoded2)\n",
        "autoencoder2.compile(optimizer='rmsprop', loss='binary_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PMK5d0IeyO9",
        "colab_type": "text"
      },
      "source": [
        "## Train AutoEncoder\n",
        "Setelah itu, mari kita latih autoencoder kedua dengan data hasil reduksi dimensi autoencoder pertama"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-K1eCc2eyO-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train2 = encoder.predict(x_train)\n",
        "autoencoder2.fit(x_train2, x_train2,\n",
        "                epochs=40,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(encoded_imgs, encoded_imgs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9Twx4oneyPB",
        "colab_type": "text"
      },
      "source": [
        "## Test Encoder \n",
        "Kemudian kita juga bangun model encoder dan decoder dari potongan autoencoder kedua"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnONhfv5eyPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder2 = Model(input_img2, encoded2)\n",
        "\n",
        "encoded_input2 = Input(shape=(encoding_dim2,))\n",
        "decoder_layer2 = autoencoder2.layers[-1]\n",
        "decoder2 = Model(encoded_input2, decoder_layer2(encoded_input2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-X8y-C4eyPF",
        "colab_type": "text"
      },
      "source": [
        "Mari kita coba kedua autoencoder kita jika digunakan untuk merekonstruksi data test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT07151YeyPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded_imgs = encoder.predict(x_test)\n",
        "encoded_imgs2 = encoder2.predict(encoded_imgs)\n",
        "decoded_imgs2 = decoder2.predict(encoded_imgs2)\n",
        "decoded_imgs = decoder.predict(decoded_imgs2)\n",
        "\n",
        "\n",
        "decoded_imgs_1 = decoder.predict(encoded_imgs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwFkGKj0eyPJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n = 10  # how many image we will display\n",
        "plt.figure(figsize=(20, 6))\n",
        "for i in range(n):\n",
        "    # display original\n",
        "    ax = plt.subplot(3, n, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28,28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display first reconstruction\n",
        "    ax = plt.subplot(3, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs_1[i].reshape(28,28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    \n",
        "    # display second reconstruction\n",
        "    ax = plt.subplot(3, n, i + 1 + n+n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28,28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSS0AUcceyPM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "autoencoder.summary()\n",
        "autoencoder2.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8Syo6nueyPP",
        "colab_type": "text"
      },
      "source": [
        "# Train Shallow Net using Pretrain\n",
        "Sekarang jika kita ingin menggunakan bobot hasil pelatihan kedua autoencoder, maka kita bangun model baru yang menggunakan kedua encoder dari autoencoder yang telah dilatih.\n",
        "\n",
        "Mari kita lihat bagaimana performanya setelah hanya 20 epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfWf0J6weyPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs2 = Input(shape=(n_input,))\n",
        "x = encoder(inputs2)\n",
        "x = encoder2(x)\n",
        "outputs2 = Dense(num_class, activation='softmax')(x)\n",
        "\n",
        "pretrain = Model(inputs2, outputs2)\n",
        "pretrain.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "pretrain.fit(x_train, y_train,\n",
        "                epochs=20,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De-PzgxaeyPS",
        "colab_type": "text"
      },
      "source": [
        "# Train Shallow Net from Scratch\n",
        "Kini, mari kita bandingkan dengan Jaringan biasa, dengan 2 layer juga, namun dengan inisialisasi bobot yang random"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbXFFAcgeyPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "inputs = Input(shape=(n_input,))\n",
        "x = Dense(500, activation='sigmoid')(inputs)\n",
        "x = Dense(200, activation='sigmoid')(x)\n",
        "outputs = Dense(num_class, activation='softmax')(x)\n",
        "\n",
        "shallow = Model(inputs, outputs)\n",
        "shallow.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "shallow.fit(x_train, y_train,\n",
        "                epochs=20,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r44qUd45eyPW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('shallow summary')\n",
        "shallow.summary()\n",
        "print()\n",
        "print('pretrain summary')\n",
        "pretrain.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGsW-TLyll4J",
        "colab_type": "text"
      },
      "source": [
        "# Shallow Scratch vs Pretrain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEueV_PyeyPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "y_pretrain = np.argmax(pretrain.predict(x_test),axis=1)\n",
        "y_shallow = np.argmax(shallow.predict(x_test),axis=1)\n",
        "\n",
        "\n",
        "print('Layer-wise Pretrain Performance')\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(np.argmax(y_test,axis=1), y_pretrain))\n",
        "print()\n",
        "\n",
        "print('Accuracy =', accuracy_score(np.argmax(y_test,axis=1), y_pretrain))\n",
        "print()\n",
        "\n",
        "print('Shallow Performance')\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(np.argmax(y_test,axis=1), y_shallow))\n",
        "print()\n",
        "\n",
        "print('Accuracy =', accuracy_score(np.argmax(y_test,axis=1), y_shallow))\n",
        "print()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EydWpzxeyPb",
        "colab_type": "text"
      },
      "source": [
        "Dapat dilihat bahwa jaringan yang sama yang dilatih dengan bobot hasil pretraining dapat menghasilkan akurasi lebih baik hanya dalam 20 epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uB_ojCQ6zmKS",
        "colab_type": "text"
      },
      "source": [
        "<p>Copyright &copy; 2019 <a href=https://www.linkedin.com/in/andityaarifianto/>ADF</a> </p>"
      ]
    }
  ]
}