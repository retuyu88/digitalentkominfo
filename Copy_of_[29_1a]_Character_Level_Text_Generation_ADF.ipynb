{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of [29 1a] Character Level Text Generation - ADF",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/retuyu88/digitalentkominfo/blob/master/Copy_of_%5B29_1a%5D_Character_Level_Text_Generation_ADF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfwbLGN0zdro",
        "colab_type": "text"
      },
      "source": [
        "<img src = \"https://i.imgur.com/UjutVJd.jpg\" align = \"center\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJBqsX-Uuyq5",
        "colab_type": "text"
      },
      "source": [
        "# Character Level Text Generation\n",
        "\n",
        "Di sini kita akan membuat language model untuk membangkitkan text dari level karakter berdasarkan input sekuens karakter yang diberikan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERD4CSQS43bS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "from tensorflow.keras.utils import get_file\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import io"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiv-1UCm-NhN",
        "colab_type": "text"
      },
      "source": [
        "# Text Data\n",
        "Untuk memulainya, kita perlu memiliki data untuk melatih model kita. Anda dapat menggunakan file teks apa pun yang Anda inginkan untuk proses ini\n",
        "\n",
        "di sini telah disediakan beberapa data text yang bisa digunakan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkhyOwmn8LtY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = {\n",
        "    'shakespeare'  : 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt',\n",
        "    'wonderland'   : 'https://www.gutenberg.org/cache/epub/11/pg11.txt',\n",
        "    'harry'        : 'https://www.linguistik.uzh.ch/dam/jcr:169bff5c-ac13-457b-9acb-4fe7f1ad5cb0/Harry%20Potter%20and%20the%20Sorcerer.txt',\n",
        "    'nietzsche'    : 'https://s3.amazonaws.com/text-datasets/nietzsche.txt',\n",
        "    'frankenstein' : 'https://www.gutenberg.org/files/84/84-0.txt'\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_bxIA-kvAeq",
        "colab_type": "text"
      },
      "source": [
        "Pilih satu data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OR-p4_t65EiT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = dataset['frankenstein']\n",
        "path = get_file( filename.split('/')[-1], origin=filename)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5TbFV1z-cN9",
        "colab_type": "text"
      },
      "source": [
        "Kita akan ubah menjadi huruf  lowercase agar kita tidak perlu khawatir tentang kapitalisasi dalam contoh ini."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzM2LVWs5RW8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "22c85358-a19e-4fa8-baf3-f7ab26c9ca4b"
      },
      "source": [
        "with io.open(path, encoding='utf-8') as f:\n",
        "    text = f.read().lower()\n",
        "print('corpus length:', len(text))\n",
        "\n",
        "\n",
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "corpus length: 440748\n",
            "﻿\n",
            "project gutenberg's frankenstein, by mary wollstonecraft (godwin) shelley\n",
            "\n",
            "this ebook is for the use of anyone anywhere at no cost and with\n",
            "almost no restrictions whatsoever.  you may copy it, give it away or\n",
            "re-use it under the terms of the projec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwLhdy2F-oyo",
        "colab_type": "text"
      },
      "source": [
        "# Encoding\n",
        "Jaringan saraf bekerja dengan angka, bukan karakter teks. Jadi kita perlu mengkonversi input karakter menjadi angka. \n",
        "\n",
        "Pertama, kita urutkan daftar unik semua karakter yang muncul dalam teks tersebut, kemudian gunakan fungsi enumerasi untuk mendapatkan angka yang mewakili karakter tersebut. \n",
        "\n",
        "Berikutnya buat kamus yang menyimpan kunci dan nilai, atau karakter dan angka yang mewakili mereka."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC5GYYNc5RT-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3dd80b58-d929-4237-a4c7-f4c22c300581"
      },
      "source": [
        "chars = sorted(list(set(text)))\n",
        "print('total chars:', len(chars))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total chars: 69\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co-Dcee-_NFE",
        "colab_type": "text"
      },
      "source": [
        "# Sequence Building\n",
        "Di sini kita set bahwa maksimum sequence dari karakter input adalah 40\n",
        "\n",
        "Untuk itu, kita harus memotong semua text dalam bentuk sekuens semi-redundan sepanjang 40 karakter. Kita gunakan nilai redundansi sebesar 3 karakter\n",
        "\n",
        "artinya, misal kita memiliki teks: `\"saya suka makan nasi\"`, kemudian kita buat sekuens semi-redundan dengan panjang 5 dan redundansi 2, maka kita akan memiliki\n",
        "* `'saya '` dengan target `'s'`\n",
        "* `'ya su'` dengan target `'k'`\n",
        "* `' suka'` dengan target `' '`\n",
        "* `'uka m'` dengan target `'a'`\n",
        "* dan seterusnya\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdiBnZdC5RRS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "338d7869-bee6-4d17-807d-93f9d7120852"
      },
      "source": [
        "# cut the text in semi-redundant sequences of maxlen characters\n",
        "maxlen = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print('nb sequences:', len(sentences))\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nb sequences: 146903\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtN3FRiwAPeF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "ea3ffd3d-0fc0-4bd2-c4a7-448ee9362705"
      },
      "source": [
        "for i in range(10):\n",
        "  print([sentences[i]],[next_chars[i]])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"\\ufeff\\nproject gutenberg's frankenstein, by m\"] ['a']\n",
            "[\"roject gutenberg's frankenstein, by mary\"] [' ']\n",
            "[\"ect gutenberg's frankenstein, by mary wo\"] ['l']\n",
            "[\" gutenberg's frankenstein, by mary wolls\"] ['t']\n",
            "[\"tenberg's frankenstein, by mary wollston\"] ['e']\n",
            "[\"berg's frankenstein, by mary wollstonecr\"] ['a']\n",
            "[\"g's frankenstein, by mary wollstonecraft\"] [' ']\n",
            "[' frankenstein, by mary wollstonecraft (g'] ['o']\n",
            "['ankenstein, by mary wollstonecraft (godw'] ['i']\n",
            "['enstein, by mary wollstonecraft (godwin)'] [' ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PjNCXnQAhMo",
        "colab_type": "text"
      },
      "source": [
        "Berikutnya kita buat data latih dan targetnya berupa vektor angka yang diambil dari dictionary berdasarkan kalimat sekuens yang sudah kita buat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Lw2Tg4253El",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9d-oAr5A9yf",
        "colab_type": "text"
      },
      "source": [
        "# LSTM Model\n",
        "Sekarang kita coba bangun jaringan sederhana mengguankan 1 layer LSTM dengan ukuran output vektor 128. Setelah layer LSTM, kita tambahkan Layer Dense untuk memprediksi kelanjutan karakter dari 40 karakter input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhIbD-8j53Bt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(maxlen, len(chars)), return_sequences=True))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "\n",
        "optimizer = Adam(lr=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYwLLUM5CPAm",
        "colab_type": "text"
      },
      "source": [
        "# Sample Probability Function\n",
        "Berikut adalah helper function untuk melakukan sampling karakter output berdasarkan output probability dari softmax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VKGcWRB52_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaKAd0M0CbAj",
        "colab_type": "text"
      },
      "source": [
        "# Training Checkpoint \n",
        "Berikutnya mari kita tambahkan sebuah callback pada fungsi training agar kita bisa melihat contoh hasil pembangkitan text sepanjang 400 karakter yang dilakukan setiap selesai melatih selama 5 epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rijJvqiB567O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def on_epoch_end(epoch, _):\n",
        "  if epoch%5==0:\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    print('\\n---------------------------------------------------------------------')\n",
        "    print('>>>>> Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    diversity = 0.7\n",
        "    print('\\n>>>>> diversity:', diversity)\n",
        "\n",
        "    generated = ''\n",
        "    sentence = text[start_index: start_index + maxlen]\n",
        "    generated += sentence\n",
        "    print('>>>>> Generating with seed: \"' + sentence + '\"')\n",
        "    sys.stdout.write(generated)\n",
        "\n",
        "    for i in range(400):\n",
        "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x_pred[0, t, char_indices[char]] = 1.\n",
        "\n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = sample(preds, diversity)\n",
        "        next_char = indices_char[next_index]\n",
        "\n",
        "        generated += next_char\n",
        "        sentence = sentence[1:] + next_char\n",
        "\n",
        "        sys.stdout.write(next_char)\n",
        "        sys.stdout.flush()\n",
        "    print('\\n---------------------------------------------------------------------')\n",
        "    print('>>>>> Continuing training')\n",
        "        \n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgscM3uqCklR",
        "colab_type": "text"
      },
      "source": [
        "# Training Process\n",
        "Sekarang tinggal kita latih model Text Generator kita"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7zmN1YM564m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        },
        "outputId": "dfff3409-6a14-468e-bf31-5b2b94682260"
      },
      "source": [
        "model.fit(x, y,\n",
        "          batch_size=512,\n",
        "          epochs=10,\n",
        "          callbacks=[print_callback])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "146432/146903 [============================>.] - ETA: 0s - loss: 2.4842\n",
            "---------------------------------------------------------------------\n",
            ">>>>> Generating text after Epoch: 0\n",
            "\n",
            ">>>>> diversity: 0.7\n",
            ">>>>> Generating with seed: \"nation, but the pertinacity with which i\"\n",
            "nation, but the pertinacity with which inn can of hindhe to and surouned.\n",
            "\n",
            "\n",
            "“is the menanger the rorred her my cenpededt in asceder dongor hid of her here me proy sis of my hander wather a pewix encencoalrs durthis when the deeeade whirite insored ext the thut and the\n",
            "wore of somere whe\n",
            "here sice and\n",
            "amares\n",
            "the hidser the mearex of thy wathers that were has with on lines of my mund and where the the fon the reace the precphed and deplin\n",
            "---------------------------------------------------------------------\n",
            ">>>>> Continuing training\n",
            "146903/146903 [==============================] - 57s 387us/sample - loss: 2.4824\n",
            "Epoch 2/10\n",
            "146903/146903 [==============================] - 37s 249us/sample - loss: 1.8857\n",
            "Epoch 3/10\n",
            "146903/146903 [==============================] - 37s 251us/sample - loss: 1.6744\n",
            "Epoch 4/10\n",
            "146903/146903 [==============================] - 37s 252us/sample - loss: 1.5355\n",
            "Epoch 5/10\n",
            "146903/146903 [==============================] - 37s 253us/sample - loss: 1.4441\n",
            "Epoch 6/10\n",
            "146432/146903 [============================>.] - ETA: 0s - loss: 1.3799\n",
            "---------------------------------------------------------------------\n",
            ">>>>> Generating text after Epoch: 5\n",
            "\n",
            ">>>>> diversity: 0.7\n",
            ">>>>> Generating with seed: \"f its original inhabitants.\n",
            "\n",
            "“these wond\"\n",
            "f its original inhabitants.\n",
            "\n",
            "“these wonders were intended or elecipation discovered my considerable to hear the lake with\n",
            "sickeness.  i could sufficall my fating-act and carried in a\n",
            "cull propised in lome fortunation, i appeared myself to your momentation, consence that you possessed my purpose on being in idrave as i was almost\n",
            "garthy beauting to armur a lay more work of the scamish and successive and well and days\n",
            "destroyed and nearer\n",
            "---------------------------------------------------------------------\n",
            ">>>>> Continuing training\n",
            "146903/146903 [==============================] - 56s 381us/sample - loss: 1.3804\n",
            "Epoch 7/10\n",
            "146903/146903 [==============================] - 37s 251us/sample - loss: 1.3279\n",
            "Epoch 8/10\n",
            "146903/146903 [==============================] - 37s 251us/sample - loss: 1.2885\n",
            "Epoch 9/10\n",
            "146903/146903 [==============================] - 37s 254us/sample - loss: 1.2523\n",
            "Epoch 10/10\n",
            "146903/146903 [==============================] - 37s 253us/sample - loss: 1.2223\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa5478b20f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD2PM4OLvku8",
        "colab_type": "text"
      },
      "source": [
        "# Testing Process\n",
        "setelah model terlatih, mari kita uji untuk membangkitkan text sepanjang 400 karakter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lII3wCV65611",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "354a849e-3c27-4bd7-9166-f34434ca826e"
      },
      "source": [
        "start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "diversity = 0.7\n",
        "print('\\n>>>>> diversity:', diversity)\n",
        "\n",
        "generated = ''\n",
        "sentence = text[start_index: start_index + maxlen]\n",
        "generated += sentence\n",
        "print('>>>>> Generating with seed: \"' + sentence + '\"')\n",
        "sys.stdout.write(generated)\n",
        "\n",
        "for i in range(400):\n",
        "    x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "    for t, char in enumerate(sentence):\n",
        "        x_pred[0, t, char_indices[char]] = 1.\n",
        "\n",
        "    preds = model.predict(x_pred, verbose=0)[0]\n",
        "    next_index = sample(preds, diversity)\n",
        "    next_char = indices_char[next_index]\n",
        "\n",
        "    generated += next_char\n",
        "    sentence = sentence[1:] + next_char\n",
        "\n",
        "    sys.stdout.write(next_char)\n",
        "    sys.stdout.flush()\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            ">>>>> diversity: 0.7\n",
            ">>>>> Generating with seed: \"t allowed to converse for any length of \"\n",
            "t allowed to converse for any length of his feet.  but share came the relations which the strouge of my awhured in its brother distruest when i had been did not also bury on the watcheds of the overhorre and letter, that i than my friends were with elizabeth, but a man, you although me them.\n",
            "\n",
            "he was the friends, she lands captain, and my eneminest hore, which had the account of life?  it is contentry or any\n",
            "probaging to breaking the pro"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uB_ojCQ6zmKS",
        "colab_type": "text"
      },
      "source": [
        "<p>Copyright &copy; 2019 <a href=https://www.linkedin.com/in/andityaarifianto/>ADF</a> </p>"
      ]
    }
  ]
}